# -*- coding: utf-8 -*-
"""tesla.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cSfVO56dWpf1n462egaa9ZGv-yZLbky_
"""

import yfinance as yf


tesla_data = yf.download('TSLA', start='2020-01-01', end='2025-06-01')


tesla_data.to_csv('tesla_stock_2020_2025.csv')

import yfinance as yf




mb_data = yf.download("MBG.DE", start="2020-01-01", end="2025-06-01")


mb_data.to_csv("mercedes_stock_2020_2025.csv")

mb_data

from google.colab import files
files.download('mercedes_stock_2020_2025.csv')

tesla_data

"""nlp data collection for tesla

"""

pip install praw

import praw
import pandas as pd
from textblob import TextBlob
from datetime import datetime


reddit = praw.Reddit(
    client_id='rCsEIXZPkqV3WwciMFo3ug',
    client_secret='2xXsGwZWvJAdQAUSusT6Ap52OsLMhg',
    user_agent='tesla_sentiment_scraper'
)

def scrape_reddit_sentiment(query, subreddits, start_date, end_date):
    posts = []
    for sub in subreddits:
        print(f"Scraping r/{sub}...")
        try:
            subreddit = reddit.subreddit(sub)
            for submission in subreddit.search(query, sort='new', time_filter='all', limit=1000):
                post_date = datetime.utcfromtimestamp(submission.created_utc).date()
                if start_date <= post_date <= end_date:
                    text = submission.title + " " + submission.selftext
                    sentiment = TextBlob(text).sentiment.polarity
                    posts.append({
                        'date': post_date,
                        'title': submission.title,
                        'text': submission.selftext,
                        'subreddit': sub,
                        'sentiment': sentiment
                    })
        except Exception as e:
            print(f"[ERROR] r/{sub}: {e}")
    return pd.DataFrame(posts)


subreddits = ['stocks', 'investing', 'wallstreetbets', 'TeslaMotors', 'electricvehicles', 'Technology', 'finance']
query = 'Tesla stock OR TSLA OR Tesla shares OR Tesla earnings OR Elon Musk'
start_date = datetime(2020, 1, 1).date()
end_date = datetime(2025, 6, 1).date()


reddit_df = scrape_reddit_sentiment(query, subreddits, start_date, end_date)
reddit_df.to_csv('tesla_reddit_sentiment.csv', index=False)

reddit_df

df = reddit_df.drop_duplicates()
df

df_1 = reddit_df.drop_duplicates(subset="date")
df_1

import praw
import pandas as pd
from textblob import TextBlob
from datetime import datetime

reddit = praw.Reddit(
    client_id='rCsEIXZPkqV3WwciMFo3ug',
    client_secret='2xXsGwZWvJAdQAUSusT6Ap52OsLMhg',
    user_agent='tesla_sentiment_scraper'
)

def scrape_reddit_sentiment(query, subreddits, start_date, end_date):
    posts = []
    for sub in subreddits:
        print(f"Scraping r/{sub}...")
        try:
            subreddit = reddit.subreddit(sub)
            for submission in subreddit.search(query, sort='new', time_filter='all', limit=1000):
                post_date = datetime.utcfromtimestamp(submission.created_utc).date()
                if start_date <= post_date <= end_date:
                    text = submission.title + " " + submission.selftext
                    sentiment = TextBlob(text).sentiment.polarity
                    posts.append({
                        'date': post_date,
                        'title': submission.title,
                        'text': submission.selftext,
                        'subreddit': sub,
                        'sentiment': sentiment
                    })
        except Exception as e:
            print(f"[ERROR] r/{sub}: {e}")
    return pd.DataFrame(posts)


subreddits = [ 'stocks', 'investing', 'wallstreetbets', 'TeslaMotors', 'electricvehicles',
    'autos', 'Futurology', 'technology', 'finance', 'Energy',
    'EVstocks', 'StockMarket', 'SustainableInvesting'
]

query =  ('Tesla OR TSLA OR "Tesla stock" OR "Tesla shares" OR "Tesla earnings" OR '
    '"Tesla delivery" OR "Tesla results" OR "Elon Musk" OR "Tesla price target"'
)

start_date = datetime(2020, 1, 1).date()
end_date = datetime(2025, 6, 1).date()


reddit_df_1 = scrape_reddit_sentiment(query, subreddits, start_date, end_date)
reddit_df_1.to_csv('tesla_reddit_sentiment.csv', index=False)

tesla_sentiment = pd.concat([reddit_df_1, reddit_df], ignore_index=True)

tesla_sentiment = tesla_sentiment.drop_duplicates()

tesla_sentiment

tesla_sentiment.to_csv('tesla_reddit_sentiment.csv', index=False)

files.download('tesla_reddit_sentiment.csv')

tesla_final= tesla_sentiment.drop_duplicates(subset="date")
tesla_final

reddit_df

final_data = reddit_df.drop_duplicates(subset="date")
final_data

combined_df = combined_df.drop_duplicates(subset='date')
combined_df

combined_df = combined_df.drop_duplicates()
combined_df

tesla_sentiment= pd.concat([df_1,final_data],axis=1)

tesla_sentiment = pd.concat([df_1, final_data]).drop_duplicates(keep=False)
tesla_sentiment

"""nlp data for mercedes"""

import praw
import pandas as pd
from datetime import datetime

reddit = praw.Reddit(
    client_id='rCsEIXZPkqV3WwciMFo3ug',
    client_secret='2xXsGwZWvJAdQAUSusT6Ap52OsLMhg',
    user_agent='mercedes_scraper_script'
)

query = (
    'Mercedes OR "Mercedes-Benz" OR Daimler OR "Mercedes EV" OR EQS OR EQE OR GLC OR GLE OR "Mercedes car" '
    'OR "Mercedes-Benz stock" OR "Mercedes company" OR "German automaker" OR "luxury cars" OR "Daimler AG"'
)


subreddits = [
    'cars', 'autos', 'automotive', 'stocks', 'investing', 'wallstreetbets',
    'electricvehicles', 'EVs', 'finance', 'technology', 'StockMarket',
    'germany', 'europe', 'luxurycars', 'mercedes_benz', 'autosuggestions',
    'GreenEnergy', 'Economics'
]

start_date = datetime(2020, 1, 1).date()
end_date   = datetime(2025, 6, 1).date()

def scrape_mercedes_posts(query, subreddits, start_date, end_date):
    posts = []

    for sub in subreddits:
        print(f"\nðŸ” Scraping r/{sub}...")
        try:
            subreddit = reddit.subreddit(sub)
            for submission in subreddit.search(query, sort='new', time_filter='all', limit=2000):
                post_date = datetime.utcfromtimestamp(submission.created_utc).date()
                if start_date <= post_date <= end_date:
                    posts.append({
                        'date': post_date,
                        'title': submission.title,
                        'text': submission.selftext,
                        'subreddit': sub,
                        'url': submission.url
                    })
        except Exception as e:
            print(f"[ERROR] r/{sub}: {e}")

    return pd.DataFrame(posts)

benz_df = scrape_mercedes_posts(query, subreddits, start_date, end_date)

benz_df.drop_duplicates(subset=["date"], inplace=True)

benz_df.to_csv("mercedes_reddit_posts.csv", index=False)

query2 = (
    '"EQS" OR "EQE" OR "GLC" OR "GLE" OR "Mercedes EQ" OR "Mercedes EVs" '
    'OR "luxury sedan" OR "German luxury car" OR "Daimler stock" OR "Mercedes future" '
    'OR "Mercedes vs BMW" OR "Mercedes earnings" OR "electric Mercedes"'
)


benz_df2 = scrape_mercedes_posts(query2, subreddits, start_date, end_date)


combined_df = pd.concat([benz_df, benz_df2], ignore_index=True)


combined_df.to_csv("mercedes_reddit_posts_final.csv", index=False)


combined_df

files.download('mercedes_reddit_posts_final.csv')



"""modeling without sentiment"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


from google.colab import files
uploaded = files.upload()

tesla_data= pd.read_csv("tesla_stock_2020_2025.csv")

print(tesla_data.head())

tesla_data.columns

tesla_data.reset_index(inplace=True)

print(tesla_data.info())

tesla_data.head()

tesla_data.info()

print(tesla_data.isnull().sum())

tesla_data.describe()

plt.figure(figsize=(12, 6))
plt.plot(tesla_data['Close'], label='Close Price')
plt.title("Tesla Stock Closing Price (2020â€“2025)")
plt.xlabel("Date")
plt.ylabel("Price (USD)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("df ets.csv")
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

"""processing for LSTM and GRU"""

scaler = MinMaxScaler()
scaled = scaler.fit_transform(df[['close']].values)

import matplotlib.pyplot as plt
import numpy as np

def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len])
    return np.array(X), np.array(y)

seq_len = 60
X, y = create_sequences(scaled, seq_len)

sample_index = 0
window = X[sample_index].flatten()
target = y[sample_index]

days = np.arange(seq_len + 1)
prices = np.append(window, target)

plt.figure(figsize=(8, 5))
plt.plot(days[:-1], window, label=" input (60 days)", color="blue", linewidth=2)
plt.scatter(days[-1], target, color="red", label="Target (61st day)", zorder=5)
plt.title("Example of Window for Sequence Generation")
plt.xlabel("Days")
plt.ylabel(" Price ")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len])
    return np.array(X), np.array(y)

seq_len = 60
X, y = create_sequences(scaled, seq_len)

split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

X_train = X_train.reshape(-1, seq_len, 1)
X_test = X_test.reshape(-1, seq_len, 1)

X_train

"""l
LSTM

"""

model = Sequential([
    LSTM(50, return_sequences=False, input_shape=(seq_len, 1)),
    Dense(1)
])
model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)
y_pred = model.predict(X_test)

residuals = y_test.flatten() - y_pred.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(y_test, label='True Price')
plt.plot(y_pred, label='LSTM Prediction')
plt.title('LSTM Forecast')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f'MSE: {mse:.4f}')
print(f'RMSE: {rmse:.4f}')

from sklearn.metrics import mean_absolute_error, r2_score

mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'MAE: {mae:.4f}')
print(f'RÂ² Score: {r2:.4f}')

"""layer 2"""

model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(seq_len, 1)))
model.add(LSTM(32))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()

model_fit= model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

pred = model.predict(X_test)

residuals = y_test.flatten() - pred.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.plot(y_test, label='Actual')
plt.plot(pred, label='Predicted')
plt.legend()
plt.title("2-Layer LSTM with Sentiment")
plt.show()

mae = mean_absolute_error(y_test, pred)
r2 = r2_score(y_test, pred)

print(f'MAE: {mae:.4f}')
print(f'RÂ² Score: {r2:.4f}')

rmse = np.sqrt(mean_squared_error(y_test, pred))
print(f'RMSE: {rmse:.4f}')

mse = mean_squared_error(y_test, pred)
print(f'MSE: {mse:.4f}')

"""bi lstm"""

from tensorflow.keras.layers import  Bidirectional

model = Sequential()
model.add(Bidirectional(LSTM(50), input_shape=(seq_len, 1)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

bi_fit = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

bi_pred = model.predict(X_test)

residuals = y_test.flatten() - bi_pred.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.plot(y_test, label='Actual')
plt.plot(bi_pred, label='Bi-LSTM Predicted')
plt.legend()
plt.title("Bi-LSTM with Sentiment")
plt.show()

rmse = np.sqrt(mean_squared_error(y_test, bi_pred))
print(f'RMSE: {rmse:.4f}')
mse = mean_squared_error(y_test, bi_pred)
print(f'MSE: {mse:.4f}')
r2 = r2_score(y_test, bi_pred)
print(f'RÂ² Score: {r2:.4f}')
mae = mean_absolute_error(y_test, bi_pred)
print(f'MAE: {mae:.4f}')

"""GRU

"""

from tensorflow.keras.layers import GRU

model = Sequential([
    GRU(50, return_sequences=False, input_shape=(seq_len, 1)),
    Dense(1)
])
model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)
y_pred = model.predict(X_test)

residuals = y_test.flatten() - y_pred.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(y_test, label='True Price')
plt.plot(y_pred, label='GRU Prediction')
plt.title('GRU Forecast')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f'MSE: {mse:.4f}')
print(f'RMSE: {rmse:.4f}')

from sklearn.metrics import mean_absolute_error, r2_score

mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'MAE: {mae:.4f}')
print(f'RÂ² Score: {r2:.4f}')

"""layer 2"""

model = Sequential()
model.add(GRU(units=50, return_sequences=True, input_shape=(seq_len, 1)))
model.add(GRU(units=32))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

gru_1 = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

pred_gru = model.predict(X_test)

residuals = y_test.flatten() - pred_gru.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.plot(y_test, label='Actual')
plt.plot(pred_gru, label='2-Layer GRU Predicted', linestyle='--')
plt.legend()
plt.title("2-Layer GRU with Sentiment")
plt.show()

rmse= np.sqrt(mean_squared_error(y_test, pred_gru))
print(f'RMSE: {rmse:.4f}')
mse = mean_squared_error(y_test, pred_gru)
print(f'MSE: {mse:.4f}')
r2 = r2_score(y_test, pred_gru)
print(f'RÂ² Score: {r2:.4f}')
mae = mean_absolute_error(y_test, pred_gru)
print(f'MAE: {mae:.4f}')

"""GRU layer 2 was best"""

results = pd.DataFrame({
    'Actual': y_test.flatten(),
    'Predicted': pred_gru.flatten()
})

results

results['Prev_Predicted'] = results['Predicted'].shift(1)
results['Signal'] = results.apply(lambda x:
    'Buy' if x['Predicted'] > x['Prev_Predicted'] * 1.005 else
    'Sell' if x['Predicted'] < x['Prev_Predicted'] * 0.995 else
    'Hold', axis=1)

results.dropna(inplace=True)


print(results[['Actual', 'Predicted', 'Signal']])

import seaborn as sns
import matplotlib.pyplot as plt


custom_palette = {
    'Buy': 'mediumseagreen',
    'Sell': 'red',
    'Hold': 'blue'   }

plt.figure(figsize=(12, 5))
sns.scatterplot(
    x=range(len(results)),
    y=results['Predicted'],
    hue=results['Signal'],
    palette=custom_palette
)
plt.title("Trading Signals Based on GRU layer 2 Forecast")
plt.xlabel("Time")
plt.ylabel("Predicted Price")
plt.legend(title='Signal')
plt.show()

import seaborn as sns
plt.figure(figsize=(12, 5))
sns.scatterplot(x=range(len(results)), y=results['Predicted'], hue=results['Signal'], palette='Set1')
plt.title("Trading Signals Based on GRU Forecast")
plt.xlabel("Time")
plt.ylabel("Predicted Price")
plt.show()

"""bi gru"""

model = Sequential()
model.add(Bidirectional(
    GRU(units=50),
    input_shape=(seq_len, 1)
))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()

bi_gru = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

bi_gru = model.predict(X_test)

residuals = y_test.flatten() - bi_gru.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
plt.plot(y_test, label='Actual')
plt.plot(bi_gru, label='Bi-GRU Predicted', linestyle='--')
plt.legend()
plt.title("Bi-directional GRU with Sentiment")
plt.show()

rmse= np.sqrt(mean_squared_error(y_test, bi_gru))
print(f'RMSE: {rmse:.4f}')
mae = mean_absolute_error(y_test, bi_gru)
print(f'MAE: {mae:.4f}')
mse = mean_squared_error(y_test, bi_gru)
print(f'MSE: {mse:.4f}')
r2 = r2_score(y_test, bi_gru)
print(f'RÂ² Score: {r2:.4f}')

"""nlp"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense

df= pd.read_csv("tesla_reddit_sentiment.csv")

df

df = df.drop(columns=['sentiment'])

df

df.info()

df['text'] = df['text'].fillna("")

"""finbert"""

df['cleaned'] = df['title'] + " " + df['text']

df

!pip install transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

model_name = "ProsusAI/finbert"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

finbert = pipeline(
    "sentiment-analysis",
    model=model,
    tokenizer=tokenizer,
    truncation=True,
    max_length=512
)

def safe(text, char_limit=2000):
    """
    Truncate text to a safe character length (approx. 512 BERT tokens)
    """
    return text[:char_limit]

df['cleaned_trunc'] = df['cleaned'].fillna('').apply(safe)



def run_finbert(text):
    try:
        result = finbert(text)[0]
        label = result['label']
        score = result['score']

        if label == 'positive':
            return +1 * score
        elif label == 'negative':
            return -1 * score
        else:
            return score
    except Exception as e:
        print(f"Error on text: {text[:30]}...: {e}")
        return 0

df['finbert_sentiment'] = df['cleaned_trunc'].apply(run_finbert)

df

"""textblob"""

from textblob import TextBlob
import re

df['cleaned'] = df['title'].astype(str)
df['text'] = df['text'].fillna('').astype(str)
df['cleaned'] = df['cleaned'] + " " + df['text']

df['cleaned']

df.info()

df['cleaned'] = df['cleaned'].apply(lambda x: re.sub("http\S+", "", x))

df['cleaned'] = df['cleaned'].apply(lambda x: re.sub("[^a-zA-Z\s]", "", x))


df['cleaned'] = df['cleaned'].apply(lambda x: x.lower())

df['cleaned'] = df['cleaned'].apply(lambda x: re.sub("\s+", " ", x).strip())


print(df[['date', 'title', 'cleaned']].head())

from nltk.stem import PorterStemmer


stemmer = PorterStemmer()


df["cleaned"] = df["cleaned"].apply(
    lambda x: " ".join([stemmer.stem(word) for word in x.split()])
)

!pip install vaderSentiment

from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

df['textblob'] = df['cleaned'].apply(lambda x: TextBlob(x).sentiment.polarity)

print(df[['date', 'title', 'textblob']])

"""vader"""

analyzer = SentimentIntensityAnalyzer()

df['vader'] = df['cleaned'].apply(lambda x: analyzer.polarity_scores(x)['compound'])

df

"""afinn"""

!pip install afinn

from afinn import Afinn

af = Afinn()
df['afinn'] = df['cleaned'].apply(lambda x: af.score(x))

df

"""bing"""

!pip install nltk

import nltk
nltk.download('opinion_lexicon')

from nltk.corpus import opinion_lexicon

positive_words = set(opinion_lexicon.positive())
negative_words = set(opinion_lexicon.negative())

def bing(text):
    words = text.split()
    pos = sum(1 for word in words if word in positive_words)
    neg = sum(1 for word in words if word in negative_words)
    return pos - neg

df['bing'] = df['cleaned'].apply(bing)

df

"""finbert"""

df.to_csv("sentinment_2.csv", index=False)

files.download("sentinment_2.csv")

"""edit"""

from google.colab import files
uploaded = files.upload()



df.isnull().sum()

df.info()

df

df['finbert_edited'] = df['finbert']

for i in range(1, len(df)):
    today_close = df.loc[i, 'close']
    today_sentiment = df.loc[i, 'finbert']
    yesterday_close = df.loc[i - 1, 'close']
    yesterday_sentiment = df.loc[i - 1, 'finbert']
    if pd.notnull(today_close) and pd.isnull(today_sentiment):
        if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
            df.loc[i, 'finbert_edited'] = yesterday_sentiment

df.isnull().sum()

df["afinn_edited"] = df["afinn"]

for i in range(1,len(df)):
  today_close= df.loc[i,"close"]
  today_sentiment=df.loc[i,"afinn"]
  yesterday_close=df.loc[i-1,"close"]
  yesterday_sentiment=df.loc[i-1,"afinn"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df.loc[i,"afinn_edited"]=yesterday_sentiment

df["bing_edited"] = df["bing"]

for i in range(1,len(df)):
  today_close= df.loc[i,"close"]
  today_sentiment=df.loc[i,"bing"]
  yesterday_close=df.loc[i-1,"close"]
  yesterday_sentiment=df.loc[i-1, "bing"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df.loc[i,"bing_edited"]=yesterday_sentiment

df["textblob_edited"] = df["textblob"]

for i in range(1,len(df)):
  today_close= df.loc[i,"close"]
  today_sentiment=df.loc[i,"textblob"]
  yesterday_close=df.loc[i-1,"close"]
  yesterday_sentiment=df.loc[i-1, "textblob"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df.loc[i,"textblob_edited"]=yesterday_sentiment

df["vader_edited"] = df["vader"]

for i in range(1,len(df)):
  today_close= df.loc[i,"close"]
  today_sentiment=df.loc[i,"vader"]
  yesterday_close=df.loc[i-1,"close"]
  yesterday_sentiment=df.loc[i-1, "vader"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df.loc[i,"vader_edited"]=yesterday_sentiment

df.isnull().sum()

"""after validating ---- finbert"""

from google.colab import files
up = files.upload()

import pandas as pd

df1=pd.read_excel("tesla final.xlsx")

df1["finbert_edited"]=df1["finbert"]

for i in range (1,len(df1)):
  today_close=df1.loc[i,"close"]
  today_sentiment=df1.loc[i,"finbert"]
  yesterday_close=df1.loc[i-1,"close"]
  yesterday_sentiment=df1.loc[i-1,"finbert"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df1.loc[i,"finbert_edited"]=yesterday_sentiment

df1

df1= df1.drop("finbert" , axis =1)

df1.to_csv("tesla_python processing.csv", index=False)

files.download("tesla_python processing.csv")

from google.colab import files
uploaded = files.upload()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_excel("tesla_final.xlsx")

corr= df.corr()

sns.heatmap(corr, annot=True)



"""mercedes

"""

from google.colab import files
df = files.upload()

import pandas as pd


df = pd.read_csv("mercedes_stock_2020_2025.csv", skiprows=2)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense

import pandas as pd


df = pd.read_csv("mercedes_full.csv", skiprows=1, header=None)

df.columns = ['date', 'close']



df

df.isnull().sum()

df

df['date'] = pd.to_datetime(df['date'], format="%d-%m-%Y")

df.info()

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,GRU,Dense
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

scaler=StandardScaler()
df['close']=scaler.fit_transform(df[['close']])

def create (data1, count):
    x= []
    y= []
    for i in range(len(data1)-count):
        x.append(data1[i:i+count])
        y.append(data1[i+count])
    return np.array(x),np.array(y)

count = 60
x ,y = create(df['close'],count)

spliting = int(len(x)*0.8)
xtrain,xtest =x[:spliting],x[spliting:]
ytrain,ytest =y[:spliting],y[spliting:]

ytest

"""LTSM mercedes

"""

model1=Sequential([LSTM(50,return_sequences=False,input_shape=(count,1)),
                   Dense(1)])

model1.compile(optimizer="adam",loss="mean_squared_error")

model1.fit(xtrain,ytrain, epochs=20,validation_data=(xtest,ytest),verbose=1)

predict=model1.predict(xtest)

residuals = ytest.flatten() - predict.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde


residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(ytest, label='True Price')
plt.plot(predict, label='LSTM Prediction')
plt.legend()

mse = mean_squared_error(ytest, predict)
rmse = np.sqrt(mse)

print(f'MSE: {mse:.4f}')
print(f'RMSE: {rmse:.4f}')

r2 = r2_score(ytest, predict)

print(f'RÂ² Score: {r2:.4f}')

"""layer 2"""

model1 = Sequential([
    LSTM(50, return_sequences=True, input_shape=(count, 1)),
    LSTM(32),
    Dense(1)
])
model1.compile(optimizer='adam', loss='mean_squared_error')

model1.fit(xtrain, ytrain, epochs=20, batch_size=32, validation_data=(xtest, ytest))

pred_lstm1 = model1.predict(xtest)

residuals = ytest.flatten() - pred_lstm1.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,6))
plt.plot(ytest, label='Actual')
plt.plot(pred_lstm1, label='2-Layer LSTM Predicted', linestyle='--')
plt.legend()

r2 = r2_score(ytest, pred_lstm1)
print(f'RÂ² Score: {r2:.4f}')
mae = mean_absolute_error(ytest, pred_lstm1)
print(f'MAE: {mae:.4f}')
mse = mean_squared_error(ytest, pred_lstm1)
print(f'MSE: {mse:.4f}')
rmse = np.sqrt(mse)
print(f'RMSE: {rmse:.4f}')

"""bi LSTM"""

from tensorflow.keras.layers import  Bidirectional

model1 = Sequential([
    Bidirectional(LSTM(50, return_sequences=False), input_shape=(count, 1)),
    Dense(1)
])
model1.compile(optimizer='adam', loss='mean_squared_error')

model1.fit(xtrain, ytrain, epochs=20, batch_size=32, validation_data=(xtest, ytest))

pred_bi= model1.predict(xtest)

residuals = ytest.flatten() - pred_bi.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,6))
plt.plot(ytest, label='Actual')
plt.plot(pred_bi, label='Bi-LSTM Predicted', linestyle='--')
plt.legend()

rmse= np.sqrt(mean_squared_error(ytest, pred_bi))
print(f'RMSE: {rmse:.4f}')
mse = mean_squared_error(ytest, pred_bi)
print(f'MSE: {mse:.4f}')
mae= mean_absolute_error(ytest, pred_bi)
print(f'MAE: {mae:.4f}')
r2 = r2_score(ytest, pred_bi)
print(f'RÂ² Score: {r2:.4f}')







"""GRU"""

model2=Sequential([GRU(50,return_sequences=False,input_shape=(count,1)),
                   Dense(1)])

model2.compile(optimizer="adam",loss="mean_squared_error")



model2.fit(xtrain,ytrain, epochs=20,validation_data=(xtest,ytest),verbose=1)

predict2=model2.predict(xtest)

residuals = ytest.flatten() - predict2.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde


residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(ytest,label='True Price')
plt.plot(predict2,label='GRU Prediction')
plt.legend()

mse1= mean_squared_error(ytest,predict2)
rmse1=np.sqrt(mse1)

print(f'MSE: {mse1:.4f}')
print(f'RMSE: {rmse1:.4f}')

r2_score(ytest,predict2)

model2 = Sequential([
    GRU(50, return_sequences=True, input_shape=(count, 1)),
    GRU(32),
    Dense(1)
])

model2.compile(optimizer='adam', loss='mean_squared_error')
model2.fit(xtrain, ytrain, epochs=20, batch_size=32, validation_data=(xtest, ytest))

pred_GRU= model2.predict(xtest)

residuals = ytest.flatten() - pred_GRU.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,6))
plt.plot(ytest, label='Actual')
plt.plot(pred_GRU, label='2-Layer GRU Predicted', linestyle='--')
plt.legend()

rmse= np.sqrt(mean_squared_error(ytest, pred_GRU))
print(f'RMSE: {rmse:.4f}')
mae = mean_absolute_error(ytest, pred_GRU)
print(f'MAE: {mae:.4f}')
mse = mean_squared_error(ytest, pred_GRU)
print(f'MSE: {mse:.4f}')
r2 = r2_score(ytest, pred_GRU)
print(f'RÂ² Score: {r2:.4f}')

"""GRU layer 2 was best"""

results = pd.DataFrame({
    'Actual': ytest.flatten(),
    'Predicted': pred_GRU.flatten()
})

results

results['Prev_Predicted'] = results['Predicted'].shift(1)
results['Signal'] = results.apply(lambda row:
    'Buy' if row['Predicted'] > row['Prev_Predicted'] * 1.005 else
    'Sell' if row['Predicted'] < row['Prev_Predicted'] * 0.995 else
    'Hold', axis=1)

results.dropna(inplace=True)


print(results[['Actual', 'Predicted', 'Signal']])

import seaborn as sns
import matplotlib.pyplot as plt


custom_palette = {
    'Buy': 'mediumseagreen',
    'Sell': 'red',
    'Hold': 'blue'   }

plt.figure(figsize=(12, 5))
sns.scatterplot(
    x=range(len(results)),
    y=results['Predicted'],
    hue=results['Signal'],
    palette=custom_palette
)
plt.title("Trading Signals Based on GRU layer 2 Forecast")
plt.xlabel("Time")
plt.ylabel("Predicted Price")
plt.legend(title='Signal')
plt.show()

"""bi directional"""

model= Sequential([
    Bidirectional(GRU(50),input_shape=(count,1)),
    Dense(1)
])
model.compile(optimizer="adam",loss="mean_squared_error")
model.fit(xtrain,ytrain, epochs=20,validation_data=(xtest,ytest),verbose=1)

pred_bi_gru=model.predict(xtest)

residuals = ytest.flatten() - pred_bi_gru.flatten()
import matplotlib.pyplot as plt
import pandas as pd


plt.figure(figsize=(12, 5))
plt.plot(residuals, label='Residuals', color='purple')
plt.axhline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Residual Plot ')
plt.xlabel('Time')
plt.ylabel('Residual (Actual - Predicted)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Residual Value')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import gaussian_kde

residuals = np.array(residuals)

kde = gaussian_kde(residuals)
x_vals = np.linspace(residuals.min(), residuals.max(), 1000)
kde_vals = kde(x_vals)

plt.figure(figsize=(7, 5))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black', alpha=0.6, density=True)
plt.plot(x_vals, kde_vals, color='darkblue', linewidth=2, label='Density Curve')
plt.axvline(0, color='red', linestyle='--', label='Zero Error Line')
plt.title('Distribution of Residuals with Density Curve')
plt.xlabel('Residual Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,6))
plt.plot(ytest, label='Actual')
plt.plot(pred_bi_gru, label='Bi-GRU Predicted', linestyle='--')
plt.legend()

rmse= np.sqrt(mean_squared_error(ytest, pred_bi_gru))
print(f'RMSE: {rmse:.4f}')
mae = mean_absolute_error(ytest, pred_bi_gru)
print(f'MAE: {mae:.4f}')
mse = mean_squared_error(ytest, pred_bi_gru)
print(f'MSE: {mse:.4f}')
r2 = r2_score(ytest, pred_bi_gru)
print(f'RÂ² Score: {r2:.4f}')

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense

df1=pd.read_csv("mercedes_reddit_posts_final.csv")

df1

df1.info()

df1["text"]=df1["text"].fillna("")

df1["cleaned"]=df1["title"]+" "+df1["text"]

df1["cleaned"]=df1["cleaned"].astype(str)

from transformers import AutoTokenizer, AutoModelForSequenceClassification,pipeline
import torch

model_name = "ProsusAI/finbert"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

finbert = pipeline("text-classification", model=model, tokenizer=tokenizer)

finbert(df1["cleaned"][0])

finbert(df1["cleaned"][2])

finbert(df1["cleaned"][4])

finbert(df1["cleaned"][10])

finbert(df1["cleaned"][8])

def safe(text, char_limit=516):
    return text[:char_limit]

df1['cleaned_trunc'] = df1['cleaned'].fillna('').apply(safe)

def run_finbert(text):
    try:
        result = finbert(text)[0]
        label = result['label']
        score = result['score']

        if label == 'positive':
            return +1 * score
        elif label == 'negative':
            return -1 * score
        else:
            return score
    except Exception as e:
        print(f"Error on text: {text}...: {e}")
        return 0

df1['finbert_sentiment'] = df1['cleaned_trunc'].apply(run_finbert)

df1





!pip install vaderSentiment

import re

"""textblob"""

df1["cleaned"]=df1["cleaned"].apply(lambda x: re.sub("http\S+", "", x))
df1["cleaned"]=df1["cleaned"].apply(lambda x: re.sub("[^a-zA-Z\s]", "", x))
df1["cleaned"]=df1["cleaned"].apply(lambda x: x.lower())
df1["cleaned"]=df1["cleaned"].apply(lambda x: re.sub("\s+", " ", x).strip())

from nltk.stem import PorterStemmer


stemmer = PorterStemmer()


df1["cleaned"] = df1["cleaned"].apply(
    lambda x: " ".join([stemmer.stem(word) for word in x.split()])
)

from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

df1["textblob"] = df1["cleaned"].apply(lambda x: TextBlob(x).sentiment.polarity)

df1["vader"]=df1["cleaned"].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x)["compound"])

df1

!pip install afinn

from afinn import Afinn

df1["afinn"]=df1["cleaned"].apply(lambda x: Afinn().score(x))

import nltk
nltk.download('opinion_lexicon')

from nltk.corpus import opinion_lexicon

positive_words = set(opinion_lexicon.positive())
negative_words = set(opinion_lexicon.negative())

def bing(text):
    words = text.split()
    pos = sum(1 for word in words if word in positive_words)
    neg = sum(1 for word in words if word in negative_words)
    return pos - neg

df1['bing'] = df1['cleaned'].apply(bing)

df1

df1.to_csv("sentinment_mercedes_1.csv", index=False)

files.download("sentinment_mercedes_1.csv")











"""preprocessing"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

df1= pd.read_excel("do preprocessing.xlsx", sheet_name ="mercedes")

df1.isnull().sum()

df1["bing_edited"]=df1["bing"]

for i in range(1,len(df1)):
  today_close= df1.loc[i,"m_close"]
  today_sentiment=df1.loc[i,"bing"]
  yesterday_close=df1.loc[i-1,"m_close"]
  yesterday_sentiment=df1.loc[i-1, "bing"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df1.loc[i,"bing_edited"]=yesterday_sentiment

df1["vader_edited"]=df1["vader"]
for i in range(1,len(df1)):
  today_close= df1.loc[i,"m_close"]
  today_sentiment=df1.loc[i,"vader"]
  yesterday_close=df1.loc[i-1,"m_close"]
  yesterday_sentiment=df1.loc[i-1, "vader"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df1.loc[i,"vader_edited"]=yesterday_sentiment

df1["finbert_edited"]=df1["finbert"]
for i in range(1,len(df1)):
  today_close= df1.loc[i,"m_close"]
  today_sentiment=df1.loc[i,"finbert"]
  yesterday_close=df1.loc[i-1,"m_close"]
  yesterday_sentiment=df1.loc[i-1, "finbert"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df1.loc[i,"finbert_edited"]=yesterday_sentiment

df1["textblob_edited"]=df1["textblob"]
for i in range(1,len(df1)):
  today_close= df1.loc[i,"m_close"]
  today_sentiment=df1.loc[i,"textblob"]
  yesterday_close=df1.loc[i-1,"m_close"]
  yesterday_sentiment=df1.loc[i-1, "textblob"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df1.loc[i,"textblob_edited"]=yesterday_sentiment

df1["afinn_edited"]=df1["afinn"]
for i in range(1,len(df1)):
  today_close= df1.loc[i,"m_close"]
  today_sentiment=df1.loc[i,"afinn"]
  yesterday_close=df1.loc[i-1,"m_close"]
  yesterday_sentiment=df1.loc[i-1, "afinn"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df1.loc[i,"afinn_edited"]=yesterday_sentiment

df1.isnull().sum()

df1=df1.drop(columns=["bing","vader","finbert","textblob","afinn"])

df1.to_csv("sentinment_mercedes_transfered_dates.csv", index=False)

files.download("sentinment_mercedes_transfered_dates.csv")

uploaded = files.upload()

df=  pd.read_excel("mercedes final.xlsx")

df["finbert_edited"]=df["finbert"]

for i in range (1,len(df)):
  today_close= df.loc[i,"m_close"]
  today_sentiment=df.loc[i,"finbert"]
  yesterday_close=df.loc[i-1,"m_close"]
  yesterday_sentiment=df.loc[i-1, "finbert"]
  if pd.notnull(today_close) and pd.isnull(today_sentiment):
    if pd.isnull(yesterday_close) and pd.notnull(yesterday_sentiment):
      df.loc[i,"finbert_edited"]=yesterday_sentiment

df= df.drop("finbert" , axis =1)

df.to_csv("mercedes_python_preprocessing.csv", index=False)

files.download("mercedes_python_preprocessing.csv")





from google.colab import files
uploaded= files.upload()

import pandas as pd

df= pd.read_excel("do labeling.xlsx", sheet_name="mecedes")

sample = df.sample(n=300, random_state=42)

sample.to_csv("mercedes_sample.csv", index=False)

files.download("mercedes_sample.csv")

df= pd.read_excel("do labeling.xlsx", sheet_name="tesla")

sample_tesla = df.sample(n=300, random_state=42)

sample_tesla

sample_tesla.to_csv("tesla_sample.csv", index=False)

files.download("tesla_sample.csv")

from google.colab import files
uploaded= files.upload()
df = pd.read_excel("labeled.xlsx")

import pandas as pd

df = pd.read_excel("labeled.xlsx")





import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, f1_score



df_clean = df.dropna(subset=['label'])

true_labels = df_clean['label'].values
scores = df_clean['finbert'].values


def find_best_thresholds(scores, true_labels):
    best_pos, best_neg = 0.1, -0.1
    best_f1 = 0
    pos_range = np.linspace(0, max(scores), 50)
    neg_range = np.linspace(min(scores), 0, 50)
    for pos_th in pos_range:
        for neg_th in neg_range:
            preds = []
            for s in scores:
                if s > pos_th:
                    preds.append('positive')
                elif s < neg_th:
                    preds.append('negative')
                else:
                    preds.append('neutral')
            f1 = f1_score(true_labels, preds, average='weighted', zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_pos = pos_th
                best_neg = neg_th
    return best_pos, best_neg, best_f1

best_pos, best_neg, best_f1 = find_best_thresholds(scores, true_labels)
print(f"Best thresholds: positive > {best_pos:.3f}, negative < {best_neg:.3f}, with F1 = {best_f1:.3f}")


def score_to_label(score):
    if score > best_pos:
        return 'positive'
    elif score < best_neg:
        return 'negative'
    else:
        return 'neutral'

preds = [score_to_label(s) for s in scores]


accuracy = accuracy_score(true_labels, preds)
precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted', zero_division=0)
cm = confusion_matrix(true_labels, preds, labels=['positive', 'neutral', 'negative'])

print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-score: {f1:.3f}")
print("Confusion Matrix:")
print(cm)

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, f1_score


df = pd.read_excel('labeled.xlsx')
df_clean = df.dropna(subset=['label'])

true_labels = df_clean['label'].values
scores = df_clean['bing'].values


def find_best_thresholds(scores, true_labels):
    best_pos, best_neg = 0.1, -0.1
    best_f1 = 0
    pos_range = np.linspace(0, max(scores), 50)
    neg_range = np.linspace(min(scores), 0, 50)
    for pos_th in pos_range:
        for neg_th in neg_range:
            preds = []
            for s in scores:
                if s > pos_th:
                    preds.append('positive')
                elif s < neg_th:
                    preds.append('negative')
                else:
                    preds.append('neutral')
            f1 = f1_score(true_labels, preds, average='weighted', zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_pos = pos_th
                best_neg = neg_th
    return best_pos, best_neg, best_f1

best_pos, best_neg, best_f1 = find_best_thresholds(scores, true_labels)
print(f"Best thresholds: positive > {best_pos:.3f}, negative < {best_neg:.3f}, with F1 = {best_f1:.3f}")


def score_to_label(score):
    if score > best_pos:
        return 'positive'
    elif score < best_neg:
        return 'negative'
    else:
        return 'neutral'

preds = [score_to_label(s) for s in scores]


accuracy = accuracy_score(true_labels, preds)
precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted', zero_division=0)
cm = confusion_matrix(true_labels, preds, labels=['positive', 'neutral', 'negative'])

print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-score: {f1:.3f}")
print("Confusion Matrix:")
print(cm)

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, f1_score


df = pd.read_excel('labeled.xlsx')
df_clean = df.dropna(subset=['label'])

true_labels = df_clean['label'].values
scores = df_clean['afinn'].values


def find_best_thresholds(scores, true_labels):
    best_pos, best_neg = 0.1, -0.1
    best_f1 = 0
    pos_range = np.linspace(0, max(scores), 50)
    neg_range = np.linspace(min(scores), 0, 50)
    for pos_th in pos_range:
        for neg_th in neg_range:
            preds = []
            for s in scores:
                if s > pos_th:
                    preds.append('positive')
                elif s < neg_th:
                    preds.append('negative')
                else:
                    preds.append('neutral')
            f1 = f1_score(true_labels, preds, average='weighted', zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_pos = pos_th
                best_neg = neg_th
    return best_pos, best_neg, best_f1

best_pos, best_neg, best_f1 = find_best_thresholds(scores, true_labels)
print(f"Best thresholds: positive > {best_pos:.3f}, negative < {best_neg:.3f}, with F1 = {best_f1:.3f}")


def score_to_label(score):
    if score > best_pos:
        return 'positive'
    elif score < best_neg:
        return 'negative'
    else:
        return 'neutral'

preds = [score_to_label(s) for s in scores]


accuracy = accuracy_score(true_labels, preds)
precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted', zero_division=0)
cm = confusion_matrix(true_labels, preds, labels=['positive', 'neutral', 'negative'])

print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-score: {f1:.3f}")
print("Confusion Matrix:")
print(cm)

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, f1_score


df = pd.read_excel('labeled.xlsx')
df_clean = df.dropna(subset=['label'])

true_labels = df_clean['label'].values
scores = df_clean['vader'].values


def find_best_thresholds(scores, true_labels):
    best_pos, best_neg = 0.1, -0.1
    best_f1 = 0
    pos_range = np.linspace(0, max(scores), 50)
    neg_range = np.linspace(min(scores), 0, 50)
    for pos_th in pos_range:
        for neg_th in neg_range:
            preds = []
            for s in scores:
                if s > pos_th:
                    preds.append('positive')
                elif s < neg_th:
                    preds.append('negative')
                else:
                    preds.append('neutral')
            f1 = f1_score(true_labels, preds, average='weighted', zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_pos = pos_th
                best_neg = neg_th
    return best_pos, best_neg, best_f1

best_pos, best_neg, best_f1 = find_best_thresholds(scores, true_labels)
print(f"Best thresholds: positive > {best_pos:.3f}, negative < {best_neg:.3f}, with F1 = {best_f1:.3f}")


def score_to_label(score):
    if score > best_pos:
        return 'positive'
    elif score < best_neg:
        return 'negative'
    else:
        return 'neutral'

preds = [score_to_label(s) for s in scores]


accuracy = accuracy_score(true_labels, preds)
precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted', zero_division=0)
cm = confusion_matrix(true_labels, preds, labels=['positive', 'neutral', 'negative'])

print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-score: {f1:.3f}")
print("Confusion Matrix:")
print(cm)

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, f1_score


df = pd.read_excel('labeled.xlsx')
df_clean = df.dropna(subset=['label'])

true_labels = df_clean['label'].values
scores = df_clean['textblob'].values


def find_best_thresholds(scores, true_labels):
    best_pos, best_neg = 0.1, -0.1
    best_f1 = 0
    pos_range = np.linspace(0, max(scores), 50)
    neg_range = np.linspace(min(scores), 0, 50)
    for pos_th in pos_range:
        for neg_th in neg_range:
            preds = []
            for s in scores:
                if s > pos_th:
                    preds.append('positive')
                elif s < neg_th:
                    preds.append('negative')
                else:
                    preds.append('neutral')
            f1 = f1_score(true_labels, preds, average='weighted', zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_pos = pos_th
                best_neg = neg_th
    return best_pos, best_neg, best_f1

best_pos, best_neg, best_f1 = find_best_thresholds(scores, true_labels)
print(f"Best thresholds: positive > {best_pos:.3f}, negative < {best_neg:.3f}, with F1 = {best_f1:.3f}")


def score_to_label(score):
    if score > best_pos:
        return 'positive'
    elif score < best_neg:
        return 'negative'
    else:
        return 'neutral'

preds = [score_to_label(s) for s in scores]


accuracy = accuracy_score(true_labels, preds)
precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted', zero_division=0)
cm = confusion_matrix(true_labels, preds, labels=['positive', 'neutral', 'negative'])

print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-score: {f1:.3f}")
print("Confusion Matrix:")
print(cm)

from google.colab import files
uploaded= files.upload()

import pandas as pd
import seaborn as sns

df=  pd.read_excel("mercedes_final.xlsx")

df

corr = df.corr()

sns.heatmap(corr , annot = True)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns



df = df.sort_values('date')


df['sentiment_shifted'] = df['finbert'].shift(1)


df_shifted = df.dropna(subset=['sentiment_shifted'])

corr = df_shifted[['sentiment_shifted', 'close']].corr()

print("Correlation matrix with shifted sentiment:")
print(corr)

plt.figure(figsize=(6,4))
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Correlation between Shifted Sentiment and Stock Price')
plt.show()